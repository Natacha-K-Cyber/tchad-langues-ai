# ğŸš€ Prochaines Ã©tapes aprÃ¨s l'installation

## âœ… Ce qui est fait
- âœ… VM Kali configurÃ©e (20GB RAM, 8 CPU)
- âœ… Environnement Python installÃ©
- âœ… Toutes les bibliothÃ¨ques ML installÃ©es
- âœ… Environnement virtuel activÃ©

## ğŸ“‹ Ã‰tape 1 : VÃ©rifier que tu es dans le bon rÃ©pertoire

```bash
# Tu devrais Ãªtre dans le dossier du projet
pwd
# Devrait afficher : /home/tyma/tchad-langues-ai

# Si tu n'es pas lÃ , va dedans :
cd ~/tchad-langues-ai

# VÃ©rifier que l'environnement virtuel est activÃ©
# Tu devrais voir (venv) au dÃ©but de ta ligne de commande
# Si ce n'est pas le cas :
source venv/bin/activate
```

## ğŸ“‹ Ã‰tape 2 : Collecter les donnÃ©es

### 2.1 : TÃ©lÃ©charger le lexique Morkeg Books

```bash
# TÃ©lÃ©charger le PDF du lexique Sara
python scripts/data_collection/download_morkeg.py
```

Cela va :
- TÃ©lÃ©charger le PDF depuis Morkeg Books
- Le sauvegarder dans `data/raw/morkeg/`

### 2.2 : Extraire le texte du PDF

```bash
# Extraire le texte du PDF tÃ©lÃ©chargÃ©
python scripts/data_processing/extract_pdf_text.py
```

Cela va :
- Extraire le texte du PDF
- Sauvegarder les donnÃ©es dans `data/processed/`

### 2.3 : Explorer d'autres sources (optionnel mais recommandÃ©)

```bash
# Voir les sources disponibles
python scripts/data_collection/explore_sources.py
```

## ğŸ“‹ Ã‰tape 3 : PrÃ©parer les donnÃ©es pour l'entraÃ®nement

### 3.1 : CrÃ©er le corpus d'entraÃ®nement

Une fois les donnÃ©es collectÃ©es, il faut les formater pour l'entraÃ®nement :

```bash
# CrÃ©er le script de prÃ©paration (on va le crÃ©er)
python scripts/data_processing/prepare_training_data.py
```

### 3.2 : VÃ©rifier les donnÃ©es prÃ©parÃ©es

```bash
# VÃ©rifier que les donnÃ©es sont prÃªtes
ls -lh data/training/
```

## ğŸ“‹ Ã‰tape 4 : Configurer l'entraÃ®nement

### 4.1 : VÃ©rifier la configuration

```bash
# Voir la configuration actuelle
cat config.yaml
```

### 4.2 : Ajuster les paramÃ¨tres si nÃ©cessaire

Pour 20GB RAM, la configuration devrait Ãªtre :
- `per_device_train_batch_size: 2`
- `use_quantization: true` (4-bit)
- `use_lora: true`

## ğŸ“‹ Ã‰tape 5 : Tester avec un petit modÃ¨le d'abord (RecommandÃ©)

Avant d'entraÃ®ner Mistral 7B, teste avec un modÃ¨le plus petit :

```bash
# Tester avec TinyLlama 1.1B (plus rapide, moins de RAM)
# On va crÃ©er un script de test
python scripts/training/test_small_model.py
```

## ğŸ“‹ Ã‰tape 6 : EntraÃ®ner le modÃ¨le

Une fois les donnÃ©es prÃªtes :

```bash
# Lancer l'entraÃ®nement
python scripts/training/fine_tune_llm.py
```

## ğŸ¯ Plan d'action immÃ©diat

**Commence par :**

1. **TÃ©lÃ©charger les donnÃ©es** :
   ```bash
   python scripts/data_collection/download_morkeg.py
   ```

2. **Extraire le texte** :
   ```bash
   python scripts/data_processing/extract_pdf_text.py
   ```

3. **VÃ©rifier ce qui a Ã©tÃ© collectÃ©** :
   ```bash
   ls -lh data/raw/morkeg/
   ls -lh data/processed/
   ```

## âš ï¸ Important

- Assure-toi d'avoir une **connexion internet** pour tÃ©lÃ©charger les donnÃ©es
- Le tÃ©lÃ©chargement du PDF peut prendre quelques minutes
- L'extraction du texte peut prendre 5-10 minutes selon la taille du PDF

## ğŸ“ Checklist

- [ ] Environnement virtuel activÃ©
- [ ] Dans le bon rÃ©pertoire (`~/tchad-langues-ai`)
- [ ] TÃ©lÃ©charger le lexique Morkeg Books
- [ ] Extraire le texte du PDF
- [ ] VÃ©rifier les donnÃ©es collectÃ©es
- [ ] PrÃ©parer les donnÃ©es pour l'entraÃ®nement
- [ ] Configurer l'entraÃ®nement
- [ ] Tester avec un petit modÃ¨le
- [ ] EntraÃ®ner le modÃ¨le final

## ğŸ’¡ Astuce

Si une commande Ã©choue, lis le message d'erreur et dis-moi ce qui ne va pas. Je t'aiderai Ã  corriger !

