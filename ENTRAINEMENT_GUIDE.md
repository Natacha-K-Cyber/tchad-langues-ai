# ğŸš€ Guide d'entraÃ®nement du modÃ¨le LLM

## âœ… DonnÃ©es prÃªtes

- **1923 entrÃ©es valides** âœ…
- **9417 traductions Sara** âœ…
- **4.90 variantes par mot** en moyenne âœ…

## ğŸ“‹ Avant de commencer l'entraÃ®nement

### Option 1 : Tester avec un petit modÃ¨le d'abord (RecommandÃ©)

Pour valider le pipeline avant d'entraÃ®ner Mistral 7B :

1. **Modifier `config.yaml`** :
```yaml
model:
  base_model: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"  # Plus petit, plus rapide
  use_quantization: true
  use_lora: true
```

2. **Tester l'entraÃ®nement** (plus rapide, ~30 min au lieu de plusieurs heures)

### Option 2 : EntraÃ®ner directement Mistral 7B

Pour un modÃ¨le plus performant (nÃ©cessite plus de temps et de RAM) :

1. **VÃ©rifier la configuration** dans `config.yaml`
2. **Lancer l'entraÃ®nement** (peut prendre plusieurs heures)

## ğŸ”§ Installation des dÃ©pendances ML

Sur ta VM Kali :

```bash
# Activer l'environnement virtuel
source venv/bin/activate

# Installer PyTorch (CPU version pour 20GB RAM)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu

# Installer les dÃ©pendances ML
pip install transformers datasets accelerate peft bitsandbytes sentencepiece tokenizers scipy
```

**Note** : Si tu as un GPU, installe la version CUDA de PyTorch Ã  la place.

## ğŸš€ Lancer l'entraÃ®nement

### Ã‰tape 1 : VÃ©rifier la configuration

```bash
# Voir la configuration actuelle
cat config.yaml | grep -A 20 "model:"
```

### Ã‰tape 2 : Lancer l'entraÃ®nement

```bash
# Lancer l'entraÃ®nement (cela peut prendre plusieurs heures)
python scripts/training/fine_tune_llm.py
```

### Ã‰tape 3 : Surveiller l'entraÃ®nement

Le script affichera :
- La progression de l'entraÃ®nement
- Les mÃ©triques (loss, perplexity)
- Les sauvegardes automatiques

## âš™ï¸ Configuration recommandÃ©e pour 20GB RAM

Dans `config.yaml`, utilise :

```yaml
model:
  base_model: "mistralai/Mistral-7B-Instruct-v0.2"
  use_quantization: true  # OBLIGATOIRE pour 20GB RAM
  use_lora: true
  lora_r: 16
  lora_alpha: 32
  
training:
  per_device_train_batch_size: 2  # AdaptÃ© pour 20GB RAM
  gradient_accumulation_steps: 4  # Simule batch_size=8
  max_length: 512
  save_total_limit: 3  # Garder seulement 3 meilleurs checkpoints
```

## â±ï¸ Temps d'entraÃ®nement estimÃ©

- **TinyLlama 1.1B** : ~30-60 minutes
- **Mistral 7B** (avec LoRA + 4-bit) : ~3-6 heures
- **Sans GPU** : 2-3x plus long

## ğŸ“Š MÃ©triques Ã  surveiller

- **Loss** : Doit diminuer (idÃ©alement < 1.0)
- **Perplexity** : Doit diminuer
- **Learning rate** : Suit le warmup puis dÃ©croÃ®t

## ğŸ’¾ Espace disque nÃ©cessaire

- **ModÃ¨le de base** : ~4-5 GB
- **Checkpoints** : ~500 MB chacun (x3 = 1.5 GB)
- **Total** : ~6-7 GB

## âš ï¸ En cas de problÃ¨me

### Erreur : "Out of memory"
- RÃ©duire `per_device_train_batch_size` Ã  1
- Augmenter `gradient_accumulation_steps` Ã  8
- VÃ©rifier que `use_quantization: true`

### Erreur : "Model not found"
- VÃ©rifier la connexion internet
- Le modÃ¨le sera tÃ©lÃ©chargÃ© automatiquement depuis Hugging Face

### Erreur : "CUDA out of memory"
- Tu utilises CPU, cette erreur ne devrait pas arriver
- Si elle arrive, rÃ©duire encore le batch size

## âœ… AprÃ¨s l'entraÃ®nement

Une fois terminÃ©, le modÃ¨le sera sauvegardÃ© dans :
```
models/tchad_langues/final/
```

Tu pourras ensuite :
1. Tester le modÃ¨le
2. L'intÃ©grer dans l'application
3. GÃ©nÃ©rer des exercices dynamiques

## ğŸ¯ Prochaines Ã©tapes

1. âœ… DonnÃ©es nettoyÃ©es (1923 entrÃ©es)
2. ğŸ”„ Installer dÃ©pendances ML
3. ğŸ”„ Lancer l'entraÃ®nement
4. â³ Tester le modÃ¨le entraÃ®nÃ©
5. â³ IntÃ©grer dans l'application

